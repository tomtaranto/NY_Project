{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détection de spams SMS\n",
    "\n",
    "Nous allons appliquer un RNN sur une tâche que l'on a essayé de résoudre auparavant avec des modèles statistiques plus basiques, mais qui est depuis maîtrisée avec le Deep Learning : la **détection de spams**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role  # Permet de récupérer le rôle actuel du Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On configure le pipeline que l'on va créer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "model_package_group_name = \"spams-sms\"\n",
    "prefix = \"pipeline-spam-sms\"\n",
    "pipeline_name = \"pipeline-spam-sms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 34797  100 34797    0     0  1029k      0 --:--:-- --:--:-- --:--:-- 1029k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17.7M  100 17.7M    0     0  57.4M      0 --:--:-- --:--:-- --:--:-- 57.4M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 23.4M  100 23.4M    0     0  91.0M      0 --:--:-- --:--:-- --:--:-- 91.0M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 24.0M  100 24.0M    0     0  87.0M      0 --:--:-- --:--:-- --:--:-- 86.7M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.9M  100 25.9M    0     0  79.0M      0 --:--:-- --:--:-- --:--:-- 79.0M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 26.9M  100 26.9M    0     0  90.1M      0 --:--:-- --:--:-- --:--:-- 90.1M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.0M  100 25.0M    0     0  75.0M      0 --:--:-- --:--:-- --:--:-- 74.7M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.2M  100 20.2M    0     0  63.3M      0 --:--:-- --:--:-- --:--:-- 63.5M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 22.3M  100 22.3M    0     0  64.3M      0 --:--:-- --:--:-- --:--:-- 64.3M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 24.1M  100 24.1M    0     0  89.7M      0 --:--:-- --:--:-- --:--:-- 89.3M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.7M  100 25.7M    0     0  90.3M      0 --:--:-- --:--:-- --:--:-- 90.3M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 26.4M  100 26.4M    0     0  90.7M      0 --:--:-- --:--:-- --:--:-- 90.4M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 27.5M  100 27.5M    0     0  59.5M      0 --:--:-- --:--:-- --:--:-- 59.3M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 24.6M  100 24.6M    0     0      0      0 --:--:-- --:--:-- --:--:--     0    0     0  71.4M      0 --:--:-- --:--:-- --:--:-- 71.4M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.3M  100 20.3M    0     0  86.6M      0 --:--:-- --:--:-- --:--:-- 86.6M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 21.5M  100 21.5M    0     0  77.1M      0 --:--:-- --:--:-- --:--:-- 76.8M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 23.3M  100 23.3M    0     0  89.5M      0 --:--:-- --:--:-- --:--:-- 89.5M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.6M  100 25.6M    0     0  87.6M      0 --:--:-- --:--:-- --:--:-- 87.6M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.4M  100 25.4M    0     0  76.0M      0 --:--:-- --:--:-- --:--:-- 76.0M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 26.6M  100 2   0    0     0      0      0 --:--:-- --:--:-- --:--:--     06.6M    0     0  85.0M      0 --:--:-- --:--:-- --:--:-- 85.0M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.5M  100 25.5M    0     0  66.5M      0 --:--:-- --:--:-- --:--:-- 66.5M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.6M  100 20.6M    0     0  64.0M      0 --:--:-- --:--:-- --:--:-- 63.8M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 22.0M  100 22.0M    0     0  63.4M      0 --:--:-- --:--:-- --:--:-- 63.4M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 22.1M  100 22.1M    0     0  75.1M      0 --:--:-- --:--:-- --:--:-- 75.1M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.0M  100 25.0M    0     0  74.0M      0 --:--:-- --:--:-- --:--:-- 74.0M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.3M  100 25.3M    0     0  60.9M      0 --:--:-- --:--:-- --:--:-- 60.7M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   7363      0 --:--:-- --:--:-- --:--:--  7363\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   8678      0 --:--:-- --:--:-- --:--:--  8678\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   8678      0 --:--:-- --:--:-- --:--:--  8678\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   6942      0 --:--:-- --:--:-- --:--:--  6942\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   6075      0 --:--:-- --:--:-- --:--:--  6075\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   5170      0 --:--:-- --:--:-- --:--:--  5282\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   6567      0 --:--:-- --:--:-- --:--:--  6750\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   8379      0 --:--:-- --:--:-- --:--:--  8379\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   9000      0 --:--:-- --:--:-- --:--:--  9000\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   9000      0 --:--:-- --:--:-- --:--:--  8678\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   9000      0 --:--:-- --:--:-- --:--:--  9000\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   9720      0 --:--:-- --:--:-- --:--:--  9720\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   7363      0 --:--:-- --:--:-- --:--:--  7363\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   6230      0 --:--:-- --:--:-- --:--:--  6230\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   7838      0 --:--:-- --:--:-- --:--:--  7838\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   7838      0 --:--:-- --:--:-- --:--:--  7838\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243    0   243    0     0   7593      0 --:--:-- --:--:-- --:--:--  7593\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!mkdir -p data/primary\n",
    "# !curl -o data/spams.csv https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/deep_learning/data/spam.csv\n",
    "\n",
    "\"\"\"\n",
    "!curl -o data/raw_data_01_01.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_01_df.csv\n",
    "!curl -o data/raw_data_01_02.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_02_df.csv\n",
    "!curl -o data/raw_data_01_03.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_03_df.csv\n",
    "!curl -o data/raw_data_01_04.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_04_df.csv\n",
    "!curl -o data/raw_data_01_05.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_05_df.csv\n",
    "!curl -o data/raw_data_01_06.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_06_df.csv\n",
    "!curl -o data/raw_data_01_07.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_07_df.csv\n",
    "!curl -o data/raw_data_01_08.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_08_df.csv\n",
    "!curl -o data/raw_data_01_09.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_09_df.csv\n",
    "!curl -o data/raw_data_01_10.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_10_df.csv\n",
    "!curl -o data/raw_data_01_11.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_11_df.csv\n",
    "!curl -o data/raw_data_01_12.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_12_df.csv\n",
    "!curl -o data/raw_data_01_13.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_13_df.csv\n",
    "!curl -o data/raw_data_01_14.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_14_df.csv\n",
    "!curl -o data/raw_data_01_15.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_15_df.csv\n",
    "!curl -o data/raw_data_01_16.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_16_df.csv\n",
    "!curl -o data/raw_data_01_17.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_17_df.csv\n",
    "!curl -o data/raw_data_01_18.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_18_df.csv\n",
    "!curl -o data/raw_data_01_19.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_19_df.csv\n",
    "\"\"\"\n",
    "!curl -o data/raw_data_01_20.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_20_df.csv\n",
    "!curl -o data/raw_data_01_21.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_21_df.csv\n",
    "!curl -o data/raw_data_01_22.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_22_df.csv\n",
    "!curl -o data/raw_data_01_23.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_23_df.csv\n",
    "!curl -o data/raw_data_01_24.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_24_df.csv\n",
    "!curl -o data/raw_data_01_25.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_25_df.csv\n",
    "!curl -o data/raw_data_01_26.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_26_df.csv\n",
    "!curl -o data/raw_data_01_27.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_27_df.csv\n",
    "!curl -o data/raw_data_01_28.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_28_df.csv\n",
    "!curl -o data/raw_data_01_29.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_29_df.csv\n",
    "!curl -o data/raw_data_01_30.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_30_df.csv\n",
    "!curl -o data/raw_data_01_31.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_01_31_df.csv\n",
    "!curl -o data/raw_data_02_01.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_01_df.csv\n",
    "!curl -o data/raw_data_02_02.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_02_df.csv\n",
    "!curl -o data/raw_data_02_03.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_03_df.csv\n",
    "!curl -o data/raw_data_02_04.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_04_df.csv\n",
    "!curl -o data/raw_data_02_05.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_05_df.csv\n",
    "!curl -o data/raw_data_02_06.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_06_df.csv\n",
    "!curl -o data/raw_data_02_07.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_07_df.csv\n",
    "!curl -o data/raw_data_02_08.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_08_df.csv\n",
    "!curl -o data/raw_data_02_09.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_09_df.csv\n",
    "!curl -o data/raw_data_02_10.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_10_df.csv\n",
    "!curl -o data/raw_data_02_11.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_11_df.csv\n",
    "!curl -o data/raw_data_02_12.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_12_df.csv\n",
    "!curl -o data/raw_data_02_13.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_13_df.csv\n",
    "!curl -o data/raw_data_02_14.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_14_df.csv\n",
    "!curl -o data/raw_data_02_15.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_15_df.csv\n",
    "!curl -o data/raw_data_02_16.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_16_df.csv\n",
    "!curl -o data/raw_data_02_17.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_17_df.csv\n",
    "!curl -o data/raw_data_02_18.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_18_df.csv\n",
    "!curl -o data/raw_data_02_19.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_19_df.csv\n",
    "!curl -o data/raw_data_02_20.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_20_df.csv\n",
    "!curl -o data/raw_data_02_21.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_21_df.csv\n",
    "!curl -o data/raw_data_02_22.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_22_df.csv\n",
    "!curl -o data/raw_data_02_23.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_23_df.csv\n",
    "!curl -o data/raw_data_02_24.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_24_df.csv\n",
    "!curl -o data/raw_data_02_25.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_25_df.csv\n",
    "!curl -o data/raw_data_02_26.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_26_df.csv\n",
    "!curl -o data/raw_data_02_27.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_27_df.csv\n",
    "!curl -o data/raw_data_02_28.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_28_df.csv\n",
    "!curl -o data/raw_data_02_29.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_29_df.csv\n",
    "!curl -o data/raw_data_02_30.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_30_df.csv\n",
    "!curl -o data/raw_data_02_31.csv https://esginyprojectdailyml.s3.eu-west-3.amazonaws.com/2019_02_31_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw_data_01_01.csv\n",
      "data/raw_data_02_01.csv\n",
      "data/raw_data_01_02.csv\n",
      "data/raw_data_02_02.csv\n",
      "data/raw_data_01_03.csv\n",
      "data/raw_data_02_03.csv\n",
      "data/raw_data_01_04.csv\n",
      "data/raw_data_02_04.csv\n",
      "data/raw_data_01_05.csv\n",
      "data/raw_data_02_05.csv\n",
      "data/raw_data_01_06.csv\n",
      "data/raw_data_02_06.csv\n",
      "data/raw_data_01_07.csv\n",
      "data/raw_data_02_07.csv\n",
      "data/raw_data_01_08.csv\n",
      "data/raw_data_02_08.csv\n",
      "data/raw_data_01_09.csv\n",
      "data/raw_data_02_09.csv\n",
      "data/raw_data_01_10.csv\n",
      "data/raw_data_02_10.csv\n",
      "data/raw_data_01_11.csv\n",
      "data/raw_data_02_11.csv\n",
      "data/raw_data_01_12.csv\n",
      "data/raw_data_02_12.csv\n",
      "data/raw_data_01_13.csv\n",
      "data/raw_data_02_13.csv\n",
      "data/raw_data_01_14.csv\n",
      "data/raw_data_02_14.csv\n",
      "data/raw_data_01_15.csv\n",
      "data/raw_data_02_15.csv\n",
      "data/raw_data_01_16.csv\n",
      "data/raw_data_02_16.csv\n",
      "data/raw_data_01_17.csv\n",
      "data/raw_data_02_17.csv\n",
      "data/raw_data_01_18.csv\n",
      "data/raw_data_02_18.csv\n",
      "data/raw_data_01_19.csv\n",
      "data/raw_data_02_19.csv\n",
      "data/raw_data_01_20.csv\n",
      "data/raw_data_02_20.csv\n",
      "data/raw_data_01_21.csv\n",
      "data/raw_data_02_21.csv\n",
      "data/raw_data_01_22.csv\n",
      "data/raw_data_02_22.csv\n",
      "data/raw_data_01_23.csv\n",
      "data/raw_data_02_23.csv\n",
      "data/raw_data_01_24.csv\n",
      "data/raw_data_02_24.csv\n",
      "data/raw_data_01_25.csv\n",
      "data/raw_data_02_25.csv\n",
      "data/raw_data_01_26.csv\n",
      "data/raw_data_02_26.csv\n",
      "data/raw_data_01_27.csv\n",
      "data/raw_data_02_27.csv\n",
      "data/raw_data_01_28.csv\n",
      "data/raw_data_02_28.csv\n",
      "data/raw_data_01_29.csv\n",
      "data/raw_data_02_29.csv\n",
      "data/raw_data_01_30.csv\n",
      "data/raw_data_02_30.csv\n",
      "data/raw_data_01_31.csv\n",
      "data/raw_data_02_31.csv\n",
      "s3://sagemaker-eu-west-3-167304041891/pipeline-spam-sms/data/primary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nspams_df = pd.read_csv(\"data/spams.csv\")\\nspams_df.to_csv(f\"./data/primary.csv\", header=True, index=False)\\nrawdata_s3_prefix = \"{}/data/primary\".format(prefix)\\nraw_s3 = sagemaker_session.upload_data(path=\"./data/\", key_prefix=rawdata_s3_prefix)\\nprint(raw_s3)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "full_df = None\n",
    "for day in range(1,32):\n",
    "    for month in range(1,3):\n",
    "        fname = f\"data/raw_data_{month:02d}_{day:02d}.csv\"\n",
    "        print(fname)\n",
    "        try:\n",
    "            df = pd.read_csv(fname)\n",
    "            full_df = pd.concat([full_df,df])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "            \n",
    "full_df.to_csv(f\"./data/primary.csv\", header=True, index=False)\n",
    "rawdata_s3_prefix = \"{}/data/primary\".format(prefix)\n",
    "raw_s3 = sagemaker_session.upload_data(path=\"./data/\", key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)\n",
    "\"\"\"\n",
    "spams_df = pd.read_csv(\"data/spams.csv\")\n",
    "spams_df.to_csv(f\"./data/primary.csv\", header=True, index=False)\n",
    "rawdata_s3_prefix = \"{}/data/primary\".format(prefix)\n",
    "raw_s3 = sagemaker_session.upload_data(path=\"./data/\", key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"1D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline - Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "\n",
    "input_data = ParameterString(name=\"InputData\", default_value=raw_s3)\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=sklearn_framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-preprocess-sms\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"Preprocess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"tokenizer\", source=\"/opt/ml/processing/tokenizer\"),\n",
    "        ProcessingOutput(output_name=\"datasets\", source=\"/opt/ml/processing/datasets\")\n",
    "    ],\n",
    "    code=\"codes/preprocess.py\",\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        input_data\n",
    "    ],\n",
    "    steps=[step_process],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.create(role_arn=role)\n",
    "#pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "import time\n",
    "\n",
    "# Chemin d'output du modèle\n",
    "model_path = f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "hyperparameters = {}\n",
    "tensorflow_version = \"2.4.1\"\n",
    "python_version = \"py37\"\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    source_dir=\"codes\",\n",
    "    entry_point=\"train.py\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=tensorflow_version,\n",
    "    role=role,\n",
    "    base_job_name=\"tensorflow-train-model\",\n",
    "    output_path=model_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    py_version=python_version,\n",
    ")\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"Train\",\n",
    "    estimator=tf_estimator,\n",
    "    inputs={\n",
    "        \"datasets\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"datasets\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        input_data\n",
    "    ],\n",
    "    steps=[step_process, step_train],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.create(role_arn=role)\n",
    "#pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.sklearn.processing import ScriptProcessor\n",
    "\n",
    "tf_eval_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=tensorflow_version,\n",
    "    image_scope=\"training\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "evaluate_model_processor = ScriptProcessor(\n",
    "    role=role, \n",
    "    image_uri=tf_eval_image_uri, \n",
    "    command=['python3'], \n",
    "    instance_count=1, \n",
    "    instance_type=training_instance_type, \n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "# Use the evaluate_model_processor in a Sagemaker pipelines ProcessingStep.\n",
    "step_evaluate = ProcessingStep(\n",
    "    name=\"Evaluate\",\n",
    "    processor=evaluate_model_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"datasets\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/datasets\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"codes/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        input_data\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_evaluate],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.create(role_arn=role)\n",
    "#pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline - Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker import PipelineModel\n",
    "\n",
    "tokenizer_s3 = \"{}/tokenizer.tar.gz\".format(\n",
    "    step_process.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]  # Sortie Tokenizer\n",
    ")\n",
    "\n",
    "tokenizer_model = SKLearnModel(\n",
    "    model_data=tokenizer_s3,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point=\"codes/preprocess.py\",\n",
    "    framework_version=sklearn_framework_version\n",
    ")\n",
    "\n",
    "tf_model_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=tensorflow_version,\n",
    "    image_scope=\"inference\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "tf_model = Model(\n",
    "    image_uri=tf_model_image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    models=[tokenizer_model, tf_model],\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"Approved\")\n",
    "\n",
    "evaluation_s3_uri = \"{}/evaluation.json\".format(\n",
    "    step_evaluate.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    ")\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=evaluation_s3_uri,\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "step_register_pipeline_model = RegisterModel(\n",
    "    name=\"RegisterPipelineModel\",\n",
    "    model=pipeline_model,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.large\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    cache_config=cache_config,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\\nfrom sagemaker.workflow.condition_step import (\\n    ConditionStep,\\n    JsonGet,\\n)\\n\\naccuracy_threshold = ParameterFloat(name=\"AccuracyThreshold\", default_value=0.8)\\n\\n# Create accuracy condition to ensure the model meets performance requirements.\\n# Models with a test accuracy lower than the condition will not be registered with the model registry.\\ncond_gte = ConditionGreaterThanOrEqualTo(\\n    left=JsonGet(\\n        step=step_evaluate,\\n        property_file=evaluation_report,\\n        json_path=\"classification_metrics.accuracy.value\",\\n    ),\\n    right=accuracy_threshold,\\n)\\n\\n# Create a Sagemaker Pipelines ConditionStep, using the condition above.\\n# Enter the steps to perform if the condition returns True / False.\\nstep_cond = ConditionStep(\\n    name=\"AccOverThreshold\",\\n    conditions=[cond_gte],\\n    if_steps=[step_register_pipeline_model],\\n    else_steps=[],\\n)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "accuracy_threshold = ParameterFloat(name=\"AccuracyThreshold\", default_value=0.8)\n",
    "\n",
    "# Create accuracy condition to ensure the model meets performance requirements.\n",
    "# Models with a test accuracy lower than the condition will not be registered with the model registry.\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=step_evaluate,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=accuracy_threshold,\n",
    ")\n",
    "\n",
    "# Create a Sagemaker Pipelines ConditionStep, using the condition above.\n",
    "# Enter the steps to perform if the condition returns True / False.\n",
    "step_cond = ConditionStep(\n",
    "    name=\"AccOverThreshold\",\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[step_register_pipeline_model],\n",
    "    else_steps=[],\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        input_data,\n",
    "        model_approval_status,\n",
    "        #accuracy_threshold\n",
    "    ],\n",
    "    #steps=[step_process, step_train, step_evaluate, step_cond],\n",
    "    steps=[step_process, step_train, step_evaluate],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:eu-west-3:167304041891:pipeline/pipeline-spam-sms/execution/rvwy1pyvnl7k', sagemaker_session=<sagemaker.session.Session object at 0x7fcad7e3ba50>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.create(role_arn=role)\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:eu-west-3:167304041891:pipeline/pipeline-spam-sms',\n",
       " 'ResponseMetadata': {'RequestId': '203c22ae-4b4b-43ac-800a-8a646e2b5766',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '203c22ae-4b4b-43ac-800a-8a646e2b5766',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '85',\n",
       "   'date': 'Thu, 20 Jan 2022 12:47:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline - Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No approved ModelPackage found for ModelPackageGroup: spams-sms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spams-sms\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No approved ModelPackage found for ModelPackageGroup: spams-sms",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-eeef781d3ebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_package_group_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_approved_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_package_group_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_model_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelPackageName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ModelPackageArn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/utils.py\u001b[0m in \u001b[0;36mget_approved_package\u001b[0;34m(model_package_group_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m             )\n\u001b[1;32m     49\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Return the pmodel package arn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No approved ModelPackage found for ModelPackageGroup: spams-sms"
     ]
    }
   ],
   "source": [
    "from codes.utils import get_approved_package\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "print(model_package_group_name)\n",
    "pck = get_approved_package(model_package_group_name)\n",
    "model_description = sm_client.describe_model_package(ModelPackageName=pck['ModelPackageArn'])\n",
    "model_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "\n",
    "model_package_arn = model_description['ModelPackageArn']\n",
    "model = ModelPackage(\n",
    "    role=role, \n",
    "    model_package_arn=model_package_arn, \n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "endpoint_name = 'spams-sms-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(\"EndpointName= {}\".format(endpoint_name))\n",
    "model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un prédicteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "p = predictor.predict(\n",
    "    json.dumps({\"samples\": [\"I am going to the park.\", \"Call from 08702490080 - tells u 2 call 09066358152 to claim �5000 prize.\"]}),\n",
    "    initial_args = {\"ContentType\": \"application/json\"}\n",
    ")\n",
    "\n",
    "json.loads(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd\n",
    "!tar -czf archive.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -halt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /opt/ml/processing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-3:615547856133:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
